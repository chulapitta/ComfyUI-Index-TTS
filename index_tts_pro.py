import os
import sys
import numpy as np
import torch
import tempfile
import soundfile as sf
import time
import re
import json
from pathlib import Path
import random

# ç¡®ä¿å½“å‰ç›®å½•åœ¨å¯¼å…¥è·¯å¾„ä¸­ / Ensure current directory is in import path
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.append(current_dir)

# å¯¼å…¥TTSæ¨¡å‹ / Import TTS models
from .tts_models import IndexTTSModel
from .indextts2 import IndexTTS2Loader, IndexTTS2Engine


class IndexTTSProNode:
    """
    ComfyUIçš„IndexTTS ProèŠ‚ç‚¹ï¼Œä¸“ç”¨äºå°è¯´é˜…è¯»ï¼Œæ”¯æŒå¤šè§’è‰²è¯­éŸ³åˆæˆå’Œæƒ…æ„Ÿæ§åˆ¶
    ComfyUI IndexTTS Pro node for novel reading with multi-character voice synthesis and emotion control
    
    æ”¯æŒæ ¼å¼ / Supported format:
    <Narrator>æ—ç™½æ–‡æœ¬</Narrator>
    <Character1 emo="å¼€å¿ƒè€Œå…´å¥‹">è§’è‰²å¯¹è¯</Character1>
    <Character2 emo="æ‚²ä¼¤è€Œéš¾è¿‡">æ‚²ä¼¤çš„å¯¹è¯</Character2>
    
    æƒ…æ„Ÿæ§åˆ¶ä»…åœ¨IndexTTS-2æ¨¡å‹ä¸­å¯ç”¨ / Emotion control only available with IndexTTS-2 model
    
    æƒ…æ„Ÿæ¨¡å¼ / Emotion modes:
    1. æ˜¾å¼æƒ…æ„Ÿ / Explicit emotion: ä½¿ç”¨emoå±æ€§æŒ‡å®š / Use emo attribute
    2. è‡ªåŠ¨æƒ…æ„Ÿ / Automatic emotion: å¯ç”¨auto_emotionä»å¯¹è¯å†…å®¹è‡ªåŠ¨åˆ†æ / Enable auto_emotion for auto analysis
    3. æŠ‘åˆ¶æƒ…æ„Ÿ / Suppress emotion: ä½¿ç”¨emo=""æ˜ç¡®ç¦ç”¨æƒ…æ„Ÿ / Use emo="" to explicitly disable emotion
    
    æ”¯æŒçš„æƒ…æ„Ÿç±»å‹ / Supported emotion types:
    æ„¤æ€’(angry), é«˜å…´(happy), ææƒ§(afraid), åæ„Ÿ(disgusted), 
    æ‚²ä¼¤(sad), ä½è½(melancholic), æƒŠè®¶(surprised), è‡ªç„¶(calm)
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "structured_text": ("STRING", {"multiline": True, "default": "<Narrator>This is a sample narrative text.<Character1 emo=\"excited\">Hello!<Narrator>He said cheerfully."}),
                "narrator_audio": ("AUDIO", {"description": "æ­£æ–‡/æ—ç™½çš„å‚è€ƒéŸ³é¢‘ / Narrator reference audio"}),
                "model_version": (["Index-TTS", "IndexTTS-1.5", "IndexTTS-2"], {"default": "IndexTTS-2"}),
                "language": (["auto", "zh", "en"], {"default": "auto"}),
                "speed": ("FLOAT", {"default": 1.0, "min": 0.5, "max": 2.0, "step": 0.1}),
                "seed": ("INT", {"default": 0, "min": 0, "max": 2**32 - 1}),
            },
            "optional": {
                "character1_audio": ("AUDIO", {"description": "è§’è‰²1çš„å‚è€ƒéŸ³é¢‘ / Character 1 reference audio"}),
                "character2_audio": ("AUDIO", {"description": "è§’è‰²2çš„å‚è€ƒéŸ³é¢‘ / Character 2 reference audio"}),
                "character3_audio": ("AUDIO", {"description": "è§’è‰²3çš„å‚è€ƒéŸ³é¢‘ / Character 3 reference audio"}),
                "character4_audio": ("AUDIO", {"description": "è§’è‰²4çš„å‚è€ƒéŸ³é¢‘ / Character 4 reference audio"}),
                "character5_audio": ("AUDIO", {"description": "è§’è‰²5çš„å‚è€ƒéŸ³é¢‘ / Character 5 reference audio"}),
                "temperature": ("FLOAT", {"default": 0.8, "min": 0.1, "max": 2.0, "step": 0.05}),
                "top_p": ("FLOAT", {"default": 0.9, "min": 0.0, "max": 1.0, "step": 0.01}),
                "top_k": ("INT", {"default": 30, "min": 0, "max": 100, "step": 1}),
                "repetition_penalty": ("FLOAT", {"default": 10.0, "min": 1.0, "max": 15.0, "step": 0.1}),
                "length_penalty": ("FLOAT", {"default": 0.0, "min": -2.0, "max": 2.0, "step": 0.1}),
                "num_beams": ("INT", {"default": 3, "min": 1, "max": 10, "step": 1}),
                "max_mel_tokens": ("INT", {"default": 1500, "min": 100, "max": 3000, "step": 50}),
                "do_sample": ("BOOLEAN", {"default": False}),
                "mode": (["Auto", "Duration", "Tokens"], {"default": "Auto"}),
                "emotion_weight": ("FLOAT", {"default": 0.6, "min": 0.0, "max": 1.0, "step": 0.05, "description": "æƒ…æ„Ÿå¼ºåº¦æ§åˆ¶ / Emotion intensity control (ç°å·²ä¿®å¤ï¼Œæ”¯æŒæƒ…æ„Ÿæ–‡æœ¬æ¨¡å¼ / Now fixed, supports emotion text mode)"}),
                "auto_emotion": ("BOOLEAN", {"default": False, "description": "è‡ªåŠ¨æƒ…æ„Ÿåˆ†æ / Automatic emotion analysis from dialogue text (IndexTTS-2 only)"}),
                "pause_between_lines": ("FLOAT", {"default": 0.2, "min": 0.0, "max": 5.0, "step": 0.01, "description": "è¡Œé—´åœé¡¿æ—¶é•¿(ç§’) / Pause duration between lines (seconds)"}),
            }
        }
    
    RETURN_TYPES = ("AUDIO", "INT", "STRING", "STRING",)
    RETURN_NAMES = ("audio", "seed", "Subtitle", "SimplifiedSubtitle",)
    FUNCTION = "generate_multi_voice_speech"
    CATEGORY = "audio"
    
    def __init__(self):
        # æ ¹è·¯å¾„ / Root path
        self.models_root = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), "models")
        # å¯ç”¨æ¨¡å‹ç‰ˆæœ¬ / Available model versions
        self.model_versions = {
            "Index-TTS": os.path.join(self.models_root, "Index-TTS"),
            "IndexTTS-1.5": os.path.join(self.models_root, "IndexTTS-1.5"),
            "IndexTTS-2": os.path.join(self.models_root, "IndexTTS-2")
        }
        # é»˜è®¤ä½¿ç”¨ IndexTTS-2 ç‰ˆæœ¬ / Default to IndexTTS-2
        self.current_version = "IndexTTS-2"
        self.model_dir = self.model_versions[self.current_version]
        # V1/V1.5 æ¨¡å‹å®ä¾‹ / V1/V1.5 model instance
        self.tts_model = None
        # V2 æ¨¡å‹å®ä¾‹ / V2 model instances
        self.tts2_loader = None
        self.tts2_engine = None
        
        print(f"[IndexTTS Pro] åˆå§‹åŒ–èŠ‚ç‚¹ï¼Œå¯ç”¨æ¨¡å‹ç‰ˆæœ¬ / Initializing node, available versions: {list(self.model_versions.keys())}")
        print(f"[IndexTTS Pro] é»˜è®¤æ¨¡å‹ç›®å½• / Default model directory: {self.model_dir}")
    
    def _init_model(self, model_version="Index-TTS"):
        """åˆå§‹åŒ–TTSæ¨¡å‹ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰ / Initialize TTS model (lazy loading)
        
        Args:
            model_version: æ¨¡å‹ç‰ˆæœ¬ï¼Œé»˜è®¤ä¸º "Index-TTS" / Model version, defaults to "Index-TTS"
        """
        # å¦‚æœç‰ˆæœ¬å‘ç”Ÿå˜åŒ–æˆ–æ¨¡å‹æœªåŠ è½½ï¼Œé‡æ–°åŠ è½½æ¨¡å‹ / Reload model if version changed or not loaded
        if self.current_version != model_version or (self.tts_model is None and self.tts2_engine is None):
            # æ›´æ–°å½“å‰ç‰ˆæœ¬å’Œæ¨¡å‹ç›®å½• / Update current version and model directory
            if model_version in self.model_versions:
                self.current_version = model_version
                self.model_dir = self.model_versions[model_version]
                print(f"[IndexTTS Pro] åˆ‡æ¢åˆ°æ¨¡å‹ç‰ˆæœ¬ / Switching to model version: {model_version}, ç›®å½• / directory: {self.model_dir}")
            else:
                print(f"[IndexTTS Pro] è­¦å‘Š / Warning: æœªçŸ¥æ¨¡å‹ç‰ˆæœ¬ / Unknown model version {model_version}ï¼Œä½¿ç”¨é»˜è®¤ç‰ˆæœ¬ / using default version {self.current_version}")
            
            # å¦‚æœå·²æœ‰æ¨¡å‹ï¼Œå…ˆé‡Šæ”¾èµ„æº / Release existing model resources
            if self.tts_model is not None:
                print(f"[IndexTTS Pro] å¸è½½ç°æœ‰V1/V1.5æ¨¡å‹ / Unloading existing V1/V1.5 model...")
                self.tts_model = None
            if self.tts2_engine is not None:
                print(f"[IndexTTS Pro] å¸è½½ç°æœ‰V2æ¨¡å‹ / Unloading existing V2 model...")
                self.tts2_loader = None
                self.tts2_engine = None
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶ / Force garbage collection
            import gc
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            print(f"[IndexTTS Pro] å¼€å§‹åŠ è½½æ¨¡å‹ç‰ˆæœ¬ / Starting to load model version: {self.current_version}...")
            
            try:
                # è®°å½•å¼€å§‹åŠ è½½æ—¶é—´ / Record start time
                start_time = time.time()
                
                if model_version == "IndexTTS-2":
                    # åŠ è½½V2æ¨¡å‹ / Load V2 model
                    print(f"[IndexTTS Pro] åŠ è½½IndexTTS-2æ¨¡å‹ / Loading IndexTTS-2 model...")
                    self.tts2_loader = IndexTTS2Loader()
                    self.tts2_engine = IndexTTS2Engine(self.tts2_loader)
                    print(f"[IndexTTS Pro] IndexTTS-2æ¨¡å‹åŠ è½½å®Œæˆ / IndexTTS-2 model loaded successfully")
                else:
                    # åŠ è½½V1/V1.5æ¨¡å‹ / Load V1/V1.5 model
                    # æ£€æŸ¥å¿…è¦çš„æ¨¡å‹æ–‡ä»¶ / Check required model files
                    required_files = ["gpt.pth", "config.yaml"]
                    missing_files = []
                    for file in required_files:
                        file_path = os.path.join(self.model_dir, file)
                        if not os.path.exists(file_path):
                            missing_files.append(file)
                        else:
                            file_size = os.path.getsize(file_path) / (1024*1024)  # è½¬æ¢ä¸ºMB / Convert to MB
                            print(f"[IndexTTS Pro] æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ / Found model file: {file} ({file_size:.2f}MB)")
                    
                    if missing_files:
                        error_msg = f"æ¨¡å‹ / Model {self.current_version} ç¼ºå°‘å¿…è¦çš„æ–‡ä»¶ / missing required files: {', '.join(missing_files)}"
                        print(f"[IndexTTS Pro] é”™è¯¯ / Error: {error_msg}")
                        raise FileNotFoundError(error_msg)
                    
                    # ä½¿ç”¨tts_models.pyä¸­çš„IndexTTSModelå®ç° / Use IndexTTSModel from tts_models.py
                    self.tts_model = IndexTTSModel(model_dir=self.model_dir)
                
                # è®°å½•åŠ è½½å®Œæˆæ—¶é—´ / Record completion time
                load_time = time.time() - start_time
                print(f"[IndexTTS Pro] æ¨¡å‹ / Model {self.current_version} å·²æˆåŠŸåŠ è½½ / loaded successfullyï¼Œè€—æ—¶ / time taken: {load_time:.2f}ç§’ / seconds")
            except Exception as e:
                import traceback
                print(f"[IndexTTS Pro] åˆå§‹åŒ–æ¨¡å‹ / Model initialization {self.current_version} å¤±è´¥ / failed: {e}")
                print(f"[IndexTTS Pro] é”™è¯¯è¯¦æƒ… / Error details:")
                traceback.print_exc()
                raise RuntimeError(f"åˆå§‹åŒ–IndexTTSæ¨¡å‹ / Initialize IndexTTS model {self.current_version} å¤±è´¥ / failed: {e}")
    
    def _process_audio_input(self, audio_input):
        """å¤„ç†ComfyUIçš„éŸ³é¢‘æ ¼å¼ / Process ComfyUI audio format
        
        Args:
            audio_input: ComfyUIçš„éŸ³é¢‘æ ¼å¼ / ComfyUI audio format
            
        Returns:
            tuple: (waveform, sample_rate) å…ƒç»„ / tuple
        """
        if audio_input is None:
            return None
            
        if isinstance(audio_input, dict) and "waveform" in audio_input and "sample_rate" in audio_input:
            waveform = audio_input["waveform"]
            sample_rate = audio_input["sample_rate"]
            
            # å¦‚æœwaveformæ˜¯torch.Tensorï¼Œè½¬æ¢ä¸ºnumpy / If waveform is torch.Tensor, convert to numpy
            if isinstance(waveform, torch.Tensor):
                waveform_np = waveform.detach().cpu().numpy()
                # å¤„ç†ä¸åŒç»´åº¦ / Handle different dimensions
                if waveform_np.ndim == 3:
                    waveform_np = waveform_np[0, 0]  # [batch, channels, samples] -> [samples]
                elif waveform_np.ndim == 2:
                    waveform_np = waveform_np[0]  # [channels, samples] -> [samples]
                return (waveform_np.astype(np.float32), int(sample_rate))
                
            return (waveform, sample_rate)
            
        # å¦‚æœå·²ç»æ˜¯å…ƒç»„æ ¼å¼ / If already tuple format
        elif isinstance(audio_input, tuple) and len(audio_input) == 2:
            return audio_input
            
        # å¦‚æœéƒ½ä¸æ˜¯ï¼ŒæŠ¥é”™ / Otherwise raise error
        raise ValueError("å‚è€ƒéŸ³é¢‘æ ¼å¼ä¸æ”¯æŒï¼Œåº”ä¸º AUDIO ç±»å‹ / Reference audio format not supported, should be AUDIO type")
    
    def _parse_structured_text(self, structured_text):
        """è§£æç»“æ„åŒ–æ–‡æœ¬ / Parse structured text
        
        Args:
            structured_text: ç»“æ„åŒ–æ–‡æœ¬ï¼Œæ”¯æŒæƒ…æ„Ÿå±æ€§ / Structured text, supports emotion attributes
                           e.g. "<Narrator>This is narrative text<Character1 emo="excited">Hello!</Character1>"
            
        Returns:
            list: è§£æåçš„æ–‡æœ¬æ®µè½åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º (role, text, emotion) / List of parsed text segments, each element is (role, text, emotion)
        """
        segments = []
        # ç®€å•åŒ¹é…æ¨¡å¼ï¼Œæ•è·æ•´ä¸ªæ ‡ç­¾å’Œå†…å®¹ / Simple matching pattern to capture entire tag and content
        pattern = re.compile(r'<(Narrator|Character\d+)([^>]*)>(.*?)(?=<|$)', re.DOTALL)
        
        # æŸ¥æ‰¾æ‰€æœ‰åŒ¹é… / Find all matches
        matches = pattern.findall(structured_text)
        
        # å¦‚æœæ‰¾ä¸åˆ°ä»»ä½•åŒ¹é…ï¼Œå°†æ•´ä¸ªæ–‡æœ¬ä½œä¸ºæ—ç™½å¤„ç† / If no matches found, treat entire text as narrator
        if not matches:
            segments.append(("Narrator", structured_text.strip(), None))
        else:
            for role, attributes, text in matches:
                # åˆ†æå±æ€§éƒ¨åˆ†ï¼ŒæŸ¥æ‰¾emo="..." / Analyze attributes part, look for emo="..."
                emotion = None
                if attributes.strip():
                    emo_match = re.search(r'emo="([^"]*)"', attributes)
                    if emo_match:
                        emotion = emo_match.group(1).strip()  # å¯èƒ½æ˜¯ç©ºå­—ç¬¦ä¸²æˆ–æœ‰å†…å®¹ / Could be empty string or have content
                
                text = text.strip()
                if text:  # åªæ·»åŠ éç©ºæ–‡æœ¬ / Only add non-empty text
                    segments.append((role, text, emotion))
                    
        return segments
    
    def _concatenate_audio(self, audio_segments):
        """è¿æ¥å¤šä¸ªéŸ³é¢‘æ®µè½
        
        Args:
            audio_segments: éŸ³é¢‘æ®µè½åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º (waveform, sample_rate)
            
        Returns:
            tuple: è¿æ¥åçš„ (waveform, sample_rate)
        """
        if not audio_segments:
            return None
            
        # ç¡®ä¿æ‰€æœ‰æ®µè½çš„é‡‡æ ·ç‡ç›¸åŒ
        sample_rate = audio_segments[0][1]
        
        # è¿‡æ»¤æœ‰æ•ˆçš„éŸ³é¢‘æ®µè½
        valid_segments = []
        for idx, segment in enumerate(audio_segments):
            try:
                audio_data, seg_sample_rate = segment
                
                # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„éŸ³é¢‘æ®µï¼Œè®¾ç½®é‡‡æ ·ç‡
                if not valid_segments:
                    sample_rate = seg_sample_rate
                    
                # æ£€æŸ¥éŸ³é¢‘æ•°æ®æ˜¯å¦æœ‰æ•ˆ
                if audio_data is not None and isinstance(audio_data, np.ndarray):
                    # ç¡®ä¿æ˜¯æœ‰æ•ˆçš„numpyæ•°ç»„
                    if audio_data.size > 0:
                        valid_segments.append(audio_data)
                        print(f"[IndexTTS Pro] Added segment {idx+1}: shape={audio_data.shape}, dtype={audio_data.dtype}")
                    else:
                        print(f"[IndexTTS Pro] Warning: Skipping empty audio segment {idx+1} with shape: {audio_data.shape}")
                else:
                    # æ‰“å°æ•°æ®ç±»å‹ä¿¡æ¯ä»¥ä¾¿è°ƒè¯•
                    print(f"[IndexTTS Pro] Warning: Skipping invalid audio data of type: {type(audio_data)}")
                    if hasattr(audio_data, '__dict__'):
                        print(f"[IndexTTS Pro] Data attributes: {dir(audio_data)}")
                    print(f"[IndexTTS Pro] Data value: {str(audio_data)[:100]}...")
            except Exception as e:
                print(f"[IndexTTS Pro] Error processing segment {idx+1}: {e}")
        
        if not valid_segments:
            print("[IndexTTS Pro] Error: No valid audio segments to concatenate")
            return None
            
        # è¿æ¥æ‰€æœ‰æœ‰æ•ˆçš„éŸ³é¢‘æ®µè½
        try:
            # è¿æ¥æ‰€æœ‰æ®µè½
            print(f"[IndexTTS Pro] Concatenating {len(valid_segments)} audio segments")
            concatenated = np.concatenate(valid_segments, axis=0)
            print(f"[IndexTTS Pro] Concatenated audio shape: {concatenated.shape}")
            
            # ç¡®ä¿éŸ³é¢‘æ•°æ®æ˜¯é€‚å½“çš„æ ¼å¼
            if concatenated.ndim == 1:
                # ä¿æŒä¸º1Dæ ¼å¼ï¼Œæˆ‘ä»¬åœ¨è¿”å›å‰ä¼šè½¬åŒ–ä¸ºé€‚å½“çš„ç»´åº¦
                print(f"[IndexTTS Pro] Audio is 1D array with {len(concatenated)} samples")
            elif concatenated.ndim > 2:
                # å¦‚æœç»´åº¦è¿‡å¤šï¼Œè½¬ä¸º1Dæ•°ç»„
                print(f"[IndexTTS Pro] Audio has too many dimensions: {concatenated.shape}, flattening")
                concatenated = concatenated.flatten()
                print(f"[IndexTTS Pro] Flattened to: {concatenated.shape}")
                
            return (concatenated, sample_rate)
        except Exception as e:
            print(f"[IndexTTS Pro] Error concatenating audio segments: {e}")
            import traceback
            traceback.print_exc()
            
            # å¦‚æœè¿æ¥å¤±è´¥ï¼Œè¿”å›ç¬¬ä¸€ä¸ªæœ‰æ•ˆæ®µè½
            if valid_segments:
                print(f"[IndexTTS Pro] Falling back to first valid segment")
                first_segment = valid_segments[0]
                return (first_segment, sample_rate)
            
            print(f"[IndexTTS Pro] No valid segments found, returning None")
            return None
    
    def _seconds_to_time_format(self, seconds):
        """å°†ç§’æ•°è½¬æ¢ä¸ºåˆ†:ç§’.æ¯«ç§’æ ¼å¼
        
        Args:
            seconds: ç§’æ•°(float)
            
        Returns:
            str: æ ¼å¼åŒ–çš„æ—¶é—´å­—ç¬¦ä¸²ï¼Œå¦‚ "1:23.456"
        """
        minutes = int(seconds // 60)
        remaining_seconds = seconds % 60
        seconds_int = int(remaining_seconds)
        milliseconds = int((remaining_seconds - seconds_int) * 1000)
        return f"{minutes}:{seconds_int:02d}.{milliseconds:03d}"
        
    def _parse_time_format(self, time_str):
        """å°†æ—¶é—´å­—ç¬¦ä¸²è½¬æ¢ä¸ºç§’æ•°
        
        Args:
            time_str: æ—¶é—´å­—ç¬¦ä¸²ï¼Œå¦‚ "1:23.456" æˆ– "1:23"
            
        Returns:
            float: å¯¹åº”çš„ç§’æ•°
        """
        # æ”¯æŒå¸¦æ¯«ç§’å’Œä¸å¸¦æ¯«ç§’çš„æ ¼å¼
        if "." in time_str:
            # æ ¼å¼: mm:ss.sss
            time_part, ms_part = time_str.split(".")
            parts = time_part.split(":")
            if len(parts) == 2:
                minutes = int(parts[0])
                seconds = int(parts[1])
                milliseconds = int(ms_part[:3].ljust(3, '0'))  # ç¡®ä¿æ˜¯3ä½æ¯«ç§’
                return minutes * 60 + seconds + milliseconds / 1000.0
        else:
            # æ ¼å¼: mm:ss (å‘åå…¼å®¹)
            parts = time_str.split(":")
            if len(parts) == 2:
                minutes = int(parts[0])
                seconds = int(parts[1])
                return minutes * 60 + seconds
        return 0.0
    
    def generate_multi_voice_speech(self, structured_text, narrator_audio, model_version="Index-TTS", 
                                   language="auto", speed=1.0, seed=0, 
                                   character1_audio=None, character2_audio=None, character3_audio=None, 
                                   character4_audio=None, character5_audio=None,
                                   temperature=0.8, top_p=0.9, top_k=30, 
                                   repetition_penalty=10.0, length_penalty=0.0, 
                                   num_beams=3, max_mel_tokens=1500,
                                   do_sample=False, mode="Auto", emotion_weight=0.8,
                                   auto_emotion=False, pause_between_lines=0.2):
        """
        ç”Ÿæˆå¤šè§’è‰²è¯­éŸ³çš„ä¸»å‡½æ•° / Main function for generating multi-character speech
        
        å‚æ•° / Parameters:
            structured_text: ç»“æ„åŒ–æ–‡æœ¬ï¼ŒåŒ…å«è§’è‰²æ ‡ç­¾ / Structured text with character tags
            narrator_audio: æ—ç™½/æ­£æ–‡çš„å‚è€ƒéŸ³é¢‘ / Narrator reference audio
            model_version: æ¨¡å‹ç‰ˆæœ¬ / Model version
            language: è¯­è¨€è®¾ç½® / Language setting
            speed: è¯­éŸ³é€Ÿåº¦ / Speech speed
            seed: éšæœºç§å­ / Random seed
            character1_audio~character5_audio: è§’è‰²å‚è€ƒéŸ³é¢‘ / Character reference audios
            temperature: æ¸©åº¦å‚æ•° / Temperature parameter
            top_p: top_på‚æ•° / top_p parameter
            top_k: top_kå‚æ•° / top_k parameter
            repetition_penalty: é‡å¤æƒ©ç½š / Repetition penalty
            length_penalty: é•¿åº¦æƒ©ç½š / Length penalty
            num_beams: beamæ•°é‡ / Number of beams
            max_mel_tokens: æœ€å¤§mel tokenæ•° / Max mel tokens
            do_sample: æ˜¯å¦ä½¿ç”¨é‡‡æ · / Whether to use sampling (V2 only)
            mode: ç”Ÿæˆæ¨¡å¼ / Generation mode (V2 only)
            emotion_weight: æƒ…æ„Ÿå¼ºåº¦æ§åˆ¶ / Emotion intensity control (0.0-1.6, V2 only)
            auto_emotion: è‡ªåŠ¨æƒ…æ„Ÿåˆ†æ / Automatic emotion analysis from dialogue text (V2 only)
            pause_between_lines: è¡Œé—´åœé¡¿æ—¶é•¿(ç§’) / Pause duration between lines (seconds)
        """
        try:
            print(f"[IndexTTS Pro] Starting multi-voice generation with structured_text: {structured_text[:100]}...")
            print(f"[IndexTTS Pro] ä½¿ç”¨æ¨¡å‹ç‰ˆæœ¬ / Using model version: {model_version}")
            
            # ä½¿ç”¨å›ºå®šç§å­æˆ–éšæœºç§å­ / Use fixed seed or random seed
            if seed == 0:
                seed = int(time.time() * 1000) % (2**32 - 1)
            
            # åˆå§‹åŒ–æ¨¡å‹ / Initialize model
            self._init_model(model_version)
            
            # è§£æç»“æ„åŒ–æ–‡æœ¬ / Parse structured text
            parsed_text = self._parse_structured_text(structured_text)
            print(f"[IndexTTS Pro] Parsed text segments: {len(parsed_text)}")
            
            # æ„å»ºè§’è‰²éŸ³é¢‘æ˜ å°„ / Build character audio mapping
            character_audios = {}
            for i, char_audio in enumerate([character1_audio, character2_audio, character3_audio, 
                                           character4_audio, character5_audio], 1):
                if char_audio is not None:
                    character_audios[f"Character{i}"] = char_audio
            
            # ç”ŸæˆéŸ³é¢‘ç‰‡æ®µ / Generate audio segments
            audio_segments = []
            current_time = 0.0  # å½“å‰æ—¶é—´ä½ç½® / Current time position
            subtitle_data = []  # Subtitleæ•°æ®åˆ—è¡¨ / Subtitle data list
            
            # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨V2æ¨¡å‹ / Check if using V2 model
            is_v2 = (model_version == "IndexTTS-2")
            
            for segment_idx, (role, text, emotion) in enumerate(parsed_text):
                emotion_text = f" (emotion: {emotion})" if emotion else ""
                print(f"\n[IndexTTS Pro] ğŸ­ Processing: {role}{emotion_text}")
                print(f"[IndexTTS Pro] ğŸ“ Text: {text[:100]}{'...' if len(text) > 100 else ''}")
                
                # é€‰æ‹©å‚è€ƒéŸ³é¢‘ / Select reference audio
                if role == "Narrator":
                    ref_audio = narrator_audio
                elif role in character_audios:
                    ref_audio = character_audios[role]
                else:
                    # ä½¿ç”¨æ—ç™½éŸ³é¢‘ä½œä¸ºé»˜è®¤å‚è€ƒ / Use narrator audio as default
                    ref_audio = narrator_audio
                    print(f"[IndexTTS Pro] Warning: No specific audio for {role}, using narrator audio")
                
                try:
                    if is_v2:
                        # ä½¿ç”¨V2 API / Use V2 API
                        # æ³¨æ„: V2ä¸æ”¯æŒspeedå‚æ•° / Note: V2 does not support speed parameter
                        
                        # ç¡®å®šæ˜¯å¦ä½¿ç”¨æƒ…æ„Ÿåˆ†æ / Determine whether to use emotion analysis
                        use_emotion_analysis = False
                        emotion_text_input = None
                        
                        if emotion is not None:
                            # æ£€æŸ¥æ˜¯å¦æ˜¾å¼æŠ‘åˆ¶æƒ…æ„Ÿ / Check if emotion is explicitly suppressed
                            if emotion == "":
                                # æ˜¾å¼æŠ‘åˆ¶æƒ…æ„Ÿï¼šemo="" / Explicitly suppress emotion: emo=""
                                use_emotion_analysis = False
                                emotion_text_input = None
                                print(f"[IndexTTS Pro] Emotion explicitly suppressed for {role}")
                            else:
                                # æ˜¾å¼æŒ‡å®šäº†æƒ…æ„Ÿæ–‡æœ¬ / Explicit emotion text specified
                                emotion_text_input = emotion
                                use_emotion_analysis = True
                                print(f"[IndexTTS Pro] Explicit Emotion Text: '{emotion}' (weight: {emotion_weight})")
                        elif auto_emotion:
                            # è‡ªåŠ¨æƒ…æ„Ÿåˆ†ææ¨¡å¼ï¼šä½¿ç”¨å¯¹è¯æ–‡æœ¬æœ¬èº« / Automatic emotion mode: use dialogue text itself
                            emotion_text_input = None  # è®©å¼•æ“è‡ªåŠ¨ä½¿ç”¨textå‚æ•° / Let engine auto-use text parameter
                            use_emotion_analysis = True
                            print(f"[IndexTTS Pro] Automatic Emotion Analysis enabled for: '{text[:50]}...'" if len(text) > 50 else f"[IndexTTS Pro] Automatic Emotion Analysis enabled for: '{text}'")
                        else:
                            print("[IndexTTS Pro] No emotion processing")
                        
                        ref_processed = self._process_audio_input(ref_audio)
                        sr, wave, sub = self.tts2_engine.generate(
                            text=text,
                            reference_audio=ref_processed,
                            mode=mode,
                            do_sample=do_sample,
                            temperature=temperature,
                            top_p=top_p,
                            top_k=top_k,
                            num_beams=num_beams,
                            repetition_penalty=repetition_penalty,
                            length_penalty=length_penalty,
                            max_mel_tokens=max_mel_tokens,
                            emo_text=emotion_text_input,  # Noneä¼šè®©å¼•æ“è‡ªåŠ¨ä½¿ç”¨text / None lets engine auto-use text
                            emo_ref_audio=None,
                            emo_vector=None,
                            emo_weight=emotion_weight,  # ä½¿ç”¨ç”¨æˆ·è®¾å®šçš„æƒ…æ„Ÿå¼ºåº¦ / Use user-defined emotion intensity
                            use_qwen=use_emotion_analysis,  # æ ¹æ®æƒ…å†µå¯ç”¨Qwenåˆ†æ / Enable Qwen analysis based on conditions
                            verbose=True,  # å¯ç”¨è¯¦ç»†æ—¥å¿—æ˜¾ç¤ºæƒ…æ„Ÿå‘é‡ / Enable verbose logging to show emotion vectors
                            seed=seed,
                            return_subtitles=True,
                        )
                        sample_rate = sr
                        audio_data = wave
                        print(f"[IndexTTS Pro] âœ… Generated audio for {role} with emotion: '{emotion}'")
                    else:
                        # ä½¿ç”¨V1/V1.5 API / Use V1/V1.5 API
                        # æ³¨æ„: V1/V1.5ä¸æ”¯æŒæƒ…æ„Ÿæ§åˆ¶ï¼Œemotionå‚æ•°è¢«å¿½ç•¥ / Note: V1/V1.5 don't support emotion control, emotion parameter is ignored
                        if emotion:
                            print(f"[IndexTTS Pro] Warning: Emotion '{emotion}' specified but V1/V1.5 models don't support emotion control")
                        result = self.tts_model.infer(
                            reference_audio=self._process_audio_input(ref_audio),
                            text=text,
                            output_path=None,
                            language=language,
                            speed=speed,
                            verbose=False,
                            temperature=temperature,
                            top_p=top_p,
                            top_k=top_k,
                            repetition_penalty=repetition_penalty,
                            length_penalty=length_penalty,
                            num_beams=num_beams,
                            max_mel_tokens=max_mel_tokens
                        )
                        
                        # å¤„ç†è¿”å›ç»“æœ / Process return result
                        if isinstance(result, tuple) and len(result) == 2:
                            sample_rate, audio_data = result
                        else:
                            print(f"[IndexTTS Pro] Warning: Unexpected return format from V1/V1.5 model")
                            continue
                    
                    # è®¡ç®—éŸ³é¢‘é•¿åº¦ / Calculate audio length
                    if isinstance(audio_data, np.ndarray) and audio_data.size > 0:
                        if audio_data.ndim == 1:
                            audio_length = len(audio_data) / sample_rate
                        else:
                            audio_length = audio_data.shape[-1] / sample_rate
                        
                        # æ·»åŠ å­—å¹•æ•°æ® / Add subtitle data
                        start_time = self._seconds_to_time_format(current_time)
                        end_time = self._seconds_to_time_format(current_time + audio_length)
                        subtitle_item = {
                            "id": role,
                            "å­—å¹•": text,
                            "start": start_time,
                            "end": end_time
                        }
                        # å¦‚æœæœ‰æƒ…æ„Ÿä¿¡æ¯ï¼Œæ·»åŠ åˆ°å­—å¹•æ•°æ®ä¸­ / If emotion info exists, add to subtitle data
                        if emotion:
                            subtitle_item["emotion"] = emotion
                        subtitle_data.append(subtitle_item)
                        current_time += audio_length
                        
                        # æ·»åŠ åˆ°æ®µè½åˆ—è¡¨ / Add to segment list
                        audio_segments.append((audio_data, sample_rate))
                        
                        # æ·»åŠ è¡Œé—´åœé¡¿ (é™¤äº†æœ€åä¸€æ®µ) / Add pause between lines (except for last segment)
                        if pause_between_lines > 0 and segment_idx < len(parsed_text) - 1:
                            silence_samples = int(pause_between_lines * sample_rate)
                            silence = np.zeros(silence_samples, dtype=np.float32)
                            audio_segments.append((silence, sample_rate))
                            current_time += pause_between_lines
                            print(f"[IndexTTS Pro] Added {pause_between_lines}s pause after {role}")
                    else:
                        print(f"[IndexTTS Pro] Warning: Invalid audio data for {role}")
                    
                except Exception as e:
                    print(f"[IndexTTS Pro] Error generating {role} voice: {e}")
                    import traceback
                    traceback.print_exc()
                    continue
            
            # è¿æ¥æ‰€æœ‰éŸ³é¢‘ç‰‡æ®µ / Concatenate all audio segments
            final_audio = self._concatenate_audio(audio_segments)
            if final_audio is None:
                raise ValueError("Failed to generate any audio segments / æœªèƒ½ç”Ÿæˆä»»ä½•éŸ³é¢‘ç‰‡æ®µ")
            
            # è®¡ç®—éŸ³é¢‘é•¿åº¦ï¼ˆè€ƒè™‘å¯èƒ½æ˜¯2Dæ ¼å¼ï¼‰ / Calculate audio length (considering possible 2D format)
            if final_audio[0].ndim > 1:
                audio_length = final_audio[0].shape[1] / final_audio[1]
            else:
                audio_length = len(final_audio[0]) / final_audio[1]
                
            print(f"[IndexTTS Pro] Multi-voice generation complete, total length: {audio_length:.2f} seconds")
            print(f"[IndexTTS Pro] Final audio shape before processing: {final_audio[0].shape}, sample rate: {final_audio[1]}")
            
            # è½¬ä¸ºComfyUIæ ¼å¼ - éœ€è¦æ˜¯3Dæ ¼å¼: [batch, channels, samples] / Convert to ComfyUI format - needs to be 3D: [batch, channels, samples]
            audio_numpy = final_audio[0]
            
            # è½¬æ¢ä¸ºPyTorchå¼ é‡ / Convert to PyTorch tensor
            audio_tensor = torch.tensor(audio_numpy, dtype=torch.float32)
            print(f"[IndexTTS Pro] Audio tensor dimensions: {audio_tensor.dim()}")
            
            # ç¡®ä¿éŸ³é¢‘æ•°æ®æ˜¯3Då¼ é‡ [batch, channels, samples] / Ensure audio data is 3D tensor [batch, channels, samples]
            if audio_tensor.dim() == 1:
                # [samples] -> [1, 1, samples]
                audio_tensor = audio_tensor.unsqueeze(0).unsqueeze(0)
                print(f"[IndexTTS Pro] 1D tensor reshaped to 3D: [1, 1, {audio_tensor.shape[-1]}]")
            elif audio_tensor.dim() == 2:
                # [channels, samples] -> [1, channels, samples]
                audio_tensor = audio_tensor.unsqueeze(0)
                print(f"[IndexTTS Pro] 2D tensor reshaped to 3D: [1, {audio_tensor.shape[1]}, {audio_tensor.shape[2]}]")
            
            print(f"[IndexTTS Pro] Final tensor shape: {audio_tensor.shape}")
            
            # ç”ŸæˆSubtitleJSONå­—ç¬¦ä¸² / Generate SubtitleJSON string
            import json
            subtitle_json = json.dumps(subtitle_data, ensure_ascii=False, indent=2)
            print(f"[IndexTTS Pro] Generated subtitle data with {len(subtitle_data)} items")
            
            # ç”Ÿæˆç®€åŒ–å­—å¹•æ ¼å¼ (åªåŒ…å«æ—¶é—´å’Œå¤„ç†åå®é™…åˆ†å¥çš„æ–‡æœ¬ï¼Œä¸åŒ…å«è§’è‰²å) / Generate simplified subtitle format (only time and processed text, without role names)
            simplified_subtitles = []
            
            # åœ¨è¿™é‡Œï¼Œç”±äºæˆ‘ä»¬æ²¡æœ‰ç›´æ¥è·å–åˆ°TTSå¤„ç†åçš„åˆ†å¥ï¼Œéœ€è¦ä»æ¨¡å‹æ—¥å¿—ä¸­è·å–æˆ–ä½¿ç”¨ä¸€ä¸ªæ¨¡æ‹Ÿå¤„ç†
            # è¿™éƒ¨åˆ†éœ€è¦æ ¹æ®tts_models.pyä¸­çš„å®é™…å¤„ç†é€»è¾‘è°ƒæ•´
            # Here, since we don't directly get sentence splits from TTS processing, we need to get it from model logs or use simulated processing
            # This part needs adjustment based on actual processing logic in tts_models.py
            
            # æˆ‘ä»¬ä½¿ç”¨å¸¦æœ‰å†’å·çš„æ—¶é—´æ ¼å¼ / We use time format with colons
            process_timepoints = []
            current_pos = 0.0
            
            # ä¸ºæ¯ä¸ªè§’è‰²çš„æ¯å¥è¯åˆ›å»ºæ—¶é—´ç‚¹ / Create timepoints for each character's lines
            for item in subtitle_data:
                # ä½¿ç”¨åŸå§‹çš„å¸¦å†’å·æ—¶é—´æ ¼å¼ / Use original time format with colons
                start_time = item["start"]
                end_time = item["end"]
                text = item["å­—å¹•"]
                
                # æ¨¡æ‹Ÿåˆ†å¥å¤„ç† - å®é™…åº”è¯¥ä»æ¨¡å‹ä¸­è·å– / Simulate sentence splitting - should actually be obtained from model
                # è¿™é‡Œç®€å•åœ°æŒ‰æ ‡ç‚¹ç¬¦å·æ‹†åˆ† / Simply split by punctuation here
                import re
                # å°†æ–‡æœ¬æ‹†åˆ†ä¸ºå¥å­ (ä¸­æ–‡æ ‡ç‚¹å’Œè‹±æ–‡æ ‡ç‚¹) / Split text into sentences (Chinese and English punctuation)
                sentences = re.split(r'([,ï¼Œ.ã€‚!ï¼?ï¼Ÿ;ï¼›])', text)
                # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²å¹¶é‡ç»„å¥å­å’Œæ ‡ç‚¹ / Filter empty strings and recombine sentences with punctuation
                sentences = [s + next_s for s, next_s in zip(sentences[::2], sentences[1::2] + [""])] if len(sentences) > 1 else [text]
                sentences = [s for s in sentences if s.strip()]
                
                if not sentences:  # å¦‚æœæ²¡æœ‰æˆåŠŸåˆ†å¥ï¼Œå°±ä½¿ç”¨åŸå§‹æ–‡æœ¬ / If no successful split, use original text
                    sentences = [text]
                
                # è®¡ç®—æ¯ä¸ªå­å¥çš„æ—¶é•¿ / Calculate duration for each sub-sentence
                total_duration = self._parse_time_format(end_time) - self._parse_time_format(start_time)
                sentence_duration = total_duration / len(sentences) if sentences else total_duration
                
                # ä¸ºæ¯ä¸ªå­å¥ç”Ÿæˆæ—¶é—´ç‚¹ / Generate timepoints for each sub-sentence
                for i, sentence in enumerate(sentences):
                    if not sentence.strip():  # è·³è¿‡ç©ºå¥ / Skip empty sentences
                        continue
                    
                    sub_start = self._parse_time_format(start_time) + i * sentence_duration
                    sub_end = sub_start + sentence_duration
                    
                    sub_start_formatted = self._seconds_to_time_format(sub_start)
                    sub_end_formatted = self._seconds_to_time_format(sub_end)
                    
                    time_line = f">> {sub_start_formatted}-{sub_end_formatted}"
                    text_line = f">> {sentence}"
                    
                    simplified_subtitles.append(time_line)
                    simplified_subtitles.append(text_line)
            
            # è¿æ¥ä¸ºå­—ç¬¦ä¸² / Join as string
            simplified_subtitle_str = "\n".join(simplified_subtitles)
            print(f"[IndexTTS Pro] Generated simplified subtitle format with processed sentences")
            
            # æœ€ç»ˆè¿”å›ComfyUIæ ¼å¼çš„éŸ³é¢‘æ•°æ®ã€ç§å­ã€JSONå­—å¹•å’Œç®€åŒ–å­—å¹• / Finally return ComfyUI format audio data, seed, JSON subtitles and simplified subtitles
            return ({"waveform": audio_tensor, "sample_rate": final_audio[1]}, seed, subtitle_json, simplified_subtitle_str)
            
        except Exception as e:
            import traceback
            print(f"[IndexTTS Pro] Generation failed / ç”Ÿæˆå¤±è´¥: {e}")
            print(traceback.format_exc())
            raise RuntimeError(f"Multi-voice generation failed / å¤šè§’è‰²è¯­éŸ³ç”Ÿæˆå¤±è´¥: {e}")
